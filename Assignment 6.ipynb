{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <center> Using Ensemble Methods to Predict Grad School Admission <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment I focused on creating and comparing a logistic regression model, a support vector machine model, and a decision tree model on my dataset. These three models then got combined to form one ensemble method, and the results of these four models were compared. The dataset I chose to work with was on predicting the likelihood of someone getting admitted to grad school, based on factors such as GRE Score, Univerisity Rating,LOR, etc. Since my target variable, AdmitPercentage, was not a binary classifier, I had to create a new variable, LikelihoodAdmit, that was a binary classifier. I said someone had a high chance of grad school if their LikelihoodAdmit was greater than 0.7. This allowed the classes to be split roughly 50%-50% for each of the groups. I then used this column as my target variable to make predictions. For these set of models, I let the models be trained on all the feature variables that were in the dataset. \n",
    "\n",
    "I also performed the grid search algorithm on both my logistic regression and support vector mahcine pipelines in order to find the best hyperparameters for the model. After creating and fitting the models on the training set using 10 fold cross validation, I end up comparing accuracy scores on the testing sets of the four models to see which one performs best for the dataset. This should allow me to pick the model that will best predict whether or not someone has a good chance of getting admitted to a graduate school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>AdmitPercentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1.0      337.0        118.0                4.0  4.5   4.5  9.65   \n",
       "1           2.0      324.0        107.0                4.0  4.0   4.5  8.87   \n",
       "2           3.0      316.0        104.0                3.0  3.0   3.5  8.00   \n",
       "3           4.0      322.0        110.0                3.0  3.5   2.5  8.67   \n",
       "4           5.0      314.0        103.0                2.0  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "495       496.0      332.0        108.0                5.0  4.5   4.0  9.02   \n",
       "496       497.0      337.0        117.0                5.0  5.0   5.0  9.87   \n",
       "497       498.0      330.0        120.0                5.0  4.5   5.0  9.56   \n",
       "498       499.0      312.0        103.0                4.0  4.0   5.0  8.43   \n",
       "499       500.0      327.0        113.0                4.0  4.5   4.5  9.04   \n",
       "\n",
       "     Research  AdmitPercentage  \n",
       "0         1.0             0.92  \n",
       "1         1.0             0.76  \n",
       "2         1.0             0.72  \n",
       "3         1.0             0.80  \n",
       "4         0.0             0.65  \n",
       "..        ...              ...  \n",
       "495       1.0             0.87  \n",
       "496       1.0             0.96  \n",
       "497       1.0             0.93  \n",
       "498       0.0             0.73  \n",
       "499       0.0             0.84  \n",
       "\n",
       "[500 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission=pd.read_csv(\"Admission_Predict.csv\").astype(\"float\")\n",
    "admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 213, 213)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission.loc[admission[\"AdmitPercentage\"]<=0.7, \"LikelihoodAdmit\"] = 0\n",
    "admission.loc[admission[\"AdmitPercentage\"]>0.7, \"LikelihoodAdmit\"] = 1\n",
    "count0=admission.loc[:,\"LikelihoodAdmit\"].eq(1.0).sum()\n",
    "#count0=admission[['LikelihoodAdmit']].eq(1.0).sum()\n",
    "count1=admission.loc[:,\"LikelihoodAdmit\"].eq(0).sum()\n",
    "yes=len(admission[admission[\"LikelihoodAdmit\"]==0])\n",
    "count0, count1, yes\n",
    "\n",
    "#admission=admission.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from here that the two target outcomes have close to the same number of observations for each outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission=admission.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=admission[:, 0:9]\n",
    "y=admission[:, 9]\n",
    "\n",
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)\n",
    "y\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the grid search algorithm on the SVM and Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "{'svc__C': 1000.0, 'svc__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_svc=make_pipeline(StandardScaler(), SVC(random_state=1))\n",
    "param_range=[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid=[{'svc__C': param_range,\n",
    "           'svc__kernel':['linear']},\n",
    "           {'svc__C':param_range,\n",
    "            'svc__gamma':param_range,\n",
    "            'svc__kernel':['rbf'] }]\n",
    "gs=GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, refit=True)\n",
    "gs=gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9942857142857143\n",
      "{'logisticregression__C': 1000.0, 'logisticregression__multi_class': 'auto', 'logisticregression__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe_log= make_pipeline(StandardScaler(), LogisticRegression(random_state=1))\n",
    "\n",
    "param_grid=[{'logisticregression__C':param_range,\n",
    "              'logisticregression__solver':['lbfgs'],\n",
    "            'logisticregression__multi_class':['auto']}]\n",
    "\n",
    "          \n",
    "gs=GridSearchCV(estimator=pipe_log, param_grid=param_grid, scoring='accuracy', cv=10, refit=True)\n",
    "gs=gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the logistic regression and on the support vector machine models, the grid search algorithim was used in order to find the optimal hyperparameters that would be best for the two separate models. This grid search algorithm is a brute force algorithm and tests every combination of hyperparameters until it finds the best ones for the respective models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "pipe1 = make_pipeline(StandardScaler(), \n",
    "                        SVC(C=1000.0, kernel='linear',random_state=1))\n",
    "\n",
    "pipe2 =  make_pipeline(DecisionTreeClassifier(max_depth=2,\n",
    "                                             criterion='entropy',\n",
    "                                             random_state=0))\n",
    "\n",
    "pipe3 = make_pipeline(StandardScaler(), LogisticRegression(C=1000.0, random_state=1, solver='lbfgs', multi_class='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 1.0 Stdev: 0.0 [SVM]\n",
      "Accuracy: 1.0 Stdev: 0.0 [Decision tree]\n",
      "Accuracy: 0.994 Stdev: 0.011 [Logistic Regression]\n"
     ]
    }
   ],
   "source": [
    "clf_labels = ['SVM', 'Decision tree', 'Logistic Regression']\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([pipe1, pipe2, pipe3], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "    print(\"Accuracy: \" + str(round(scores.mean(), 3)) + \n",
    "          \" Stdev: \" + str(round(scores.std(), 3)) +\n",
    "          \" [\" + label + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "mv_clf = VotingClassifier(estimators=[('svm', pipe1), ('dt', pipe2), ('log', pipe3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble method I choose to use is one that utilizes majority class voting based on the three prior models. Through a combination of the support vector machine model, the decision tree model, and the logistic regression model, the ensemble model will now use majority voting with these three models to create this new ensemble method to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0 Stdev: 0.0 [SVM]\n",
      "Accuracy: 1.0 Stdev: 0.0 [Decision tree]\n",
      "Accuracy: 0.99 Stdev: 0.011 [Logistic Regression]\n",
      "Accuracy: 1.0 Stdev: 0.0 [Majority voting]\n"
     ]
    }
   ],
   "source": [
    "clf_labels += ['Majority voting']\n",
    "all_clf = [pipe1, pipe2, pipe3, mv_clf]\n",
    "\n",
    "for clf, label in zip(all_clf, clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "    print(\"Accuracy: \" + str(round(scores.mean(), 2)) + \n",
    "          \" Stdev: \" + str(round(scores.std(), 3)) +\n",
    "          \" [\" + label + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From running a 10 fold cross validation on each of the different models, we can see that the three models do an extremely good job for all four of the different models. For the SVM, Decision Tree, and the Ensemble Majority Voting methods, we can see that the accuracy is 100% with 0 standard deviation. For the Logistic Regression model we can that this one only performs marginally worse with an accuracy of 0.99 and a std deviation of 0.011. This means that each of the four models does an extremely good job on fitting the training dataset. One of the potential worries with this is that the model may have high variance and could have an over-fitting issue but we can now test our models and see how they do on the testing set to see if this ends up being true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model\n",
      "Misclassified test set examples: 0\n",
      "Out of a total of: 150\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe1.predict(X_test)\n",
    "print(\"SVM Model\")\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', pipe1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model\n",
      "Misclassified test set examples: 0\n",
      "Out of a total of: 150\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "pipe2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe2.predict(X_test)\n",
    "print(\"Decision Tree Model\")\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', pipe2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model\n",
      "Misclassified test set examples: 0\n",
      "Out of a total of: 150\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "pipe3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe3.predict(X_test)\n",
    "print(\"Logistic Regression Model\")\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', pipe3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Method Model\n",
      "Misclassified test set examples: 0\n",
      "Out of a total of: 150\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "mv_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mv_clf.predict(X_test)\n",
    "print(\"Ensemble Method Model\")\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', y_test.shape[0])\n",
    "print('Accuracy:', mv_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all four of the models, we can see that each one of them 100% perfectly classified the 150 examples in the testing data set. This means that for this dataset, either of the four models would be a great model to use. While I was wary that the models overfit the training data, we can see that the high accuracy scores still held up when we fitted them to the testing data. While this may seem suprising to have this high of an accuracy score, I think this occurred based on how I split up the dataset to begin with. If I were to change the highchance of admit to above say 80% or even 90% I believe the dataset might not have done as good of a job predicting the testing set. What these models do tell us though is that when we use all the different factor variables that are available in the dataset, that we can create a very good model that accruately predicts if someone has a high likelhood of getting into graduate school or not. Overall each of these models do a great job of predicting admit likelihood though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
